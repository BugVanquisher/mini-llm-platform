# Mini LLM Platform

A lightweight **LLM platform & infra demo** for learning and experimentation.  
This project provides a FastAPI service that wraps **LLM providers** (e.g., Ollama, OpenAI), integrates **monitoring with Prometheus/Grafana**, and supports **Retrieval-Augmented Generation (RAG)** with document ingestion and vector search (ChromaDB).

---

## âœ¨ Features

- **FastAPI service** with `/generate` endpoint for text generation  
- **Provider abstraction**: swap between Ollama (local), OpenAI (API), or others  
- **System prompt enforcement** for consistent responses  
- **Prometheus metrics**: latency, errors, token usage, cost estimates  
- **RAG pipeline**:  
  - `/rag/ingest`: upload and index documents (using ChromaDB + SentenceTransformers)  
  - `/rag/query`: retrieve relevant docs, feed them into the LLM, return grounded answers with sources  
- **Docker Compose setup** with Prometheus + Grafana dashboards  
- **Extensible**: add new models, agents, tools, or evals  

---

## ğŸ— Project Structure
```
src/
â”œâ”€â”€ app.py              # FastAPI app (endpoints, metrics, orchestration)
â”œâ”€â”€ rag.py              # RAG pipeline (ingestion, retrieval)
â”œâ”€â”€ agent.py            # Agent loop (tools + reasoning)
â”œâ”€â”€ metrics.py          # Prometheus metrics definitions
â”œâ”€â”€ inference.py        # Provider abstractions (Ollama, OpenAI, etc.)
â”œâ”€â”€ evals_rag.py        # Simple regression tests for RAG
configs/
â”œâ”€â”€ model_config.yaml   # Config for provider, model name, system prompts
docker-compose.yml      # Services: API, Prometheus, Grafana
requirements.txt        # Python dependencies
```
---

## ğŸš€ Getting Started

### 1. Clone the repo
```bash
git clone https://github.com/yourusername/mini-llm-platform.git
cd mini-llm-platform
```

### 2. Install dependencies

Using Conda or venv:
```
pip install -r requirements.txt
```

### 3. Start services with Docker Compose
```
docker-compose up --build
```
```
This will launch:
	â€¢	mini-llm-app (FastAPI LLM service at http://localhost:8000)
	â€¢	Prometheus (metrics at http://localhost:9090)
	â€¢	Grafana (dashboards at http://localhost:3000, default creds admin/admin)
```
â¸»

## ğŸ”§ Usage

Health check
```
curl http://localhost:8000/health
```
Generate text
```
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a 3-sentence summary of RAG for LLMs."}'
```
Ingest documents into RAG
```
curl -X POST "http://localhost:8000/rag/ingest" \
  -F "files=@docs/llm_notes.txt"
```
Query with RAG
```
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Retrieval-Augmented Generation?", "top_k": 3}'
```
Ask the agent
```
curl -X POST "http://localhost:8000/agent" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is 25 * 4 + 10?"}'
```
â¸»

## ğŸ“Š Monitoring
Prometheus metrics exposed at:
```
http://localhost:8000/metrics
```

Key metrics:

	â€¢	llm_request_latency_seconds (per provider/model)
	â€¢	llm_tokens_total
	â€¢	llm_cost_usd_total
	â€¢	rag_queries_total
	â€¢	rag_query_latency_seconds
    â€¢	agent_queries_total, agent_tool_invocations_total, agent_latency_seconds
	â€¢	Grafana dashboards available at http://localhost:3000

â¸»

## ğŸ§ª Testing & Evals

Run RAG evals:
```
python src/evals_rag.py
```
This will:

	â€¢	Run a few QA pairs against /rag/query
	â€¢	Check grounding against retrieved docs
	â€¢	Report pass/fail rates

Agent evals (coming soon):
```
python src/evals_agent.py
```

â¸»

## ğŸ”® Roadmap
âœ… Phase 1: Core Infra
	â€¢	FastAPI service
	â€¢	LLM providers
	â€¢	Prometheus + Grafana

âœ… Phase 2: RAG
	â€¢	Document ingestion & retrieval
	â€¢	RAG metrics + dashboards

âœ… Phase 3: Agents
	â€¢	Agent loop (ReAct-style)
	â€¢	RAG + Calculator tools
	â€¢	Agent observability

ğŸ”œ Phase 4: Multi-Agent Orchestration
	â€¢	Multi-agent collaboration (planner + worker agents)
	â€¢	Workflow orchestration (task decomposition, parallelization)
	â€¢	External connectors (databases, APIs, knowledge graphs)
	â€¢	Agent evaluation & safety guardrails
â¸»

## ğŸ“œ License

MIT License. Feel free to use and modify for your own experiments.

---

âš¡ Now you can save this as `README.md` at the root of your repo.  

Would you like me to also generate a **ready-to-import Grafana dashboard JSON** thatâ€™s prewired to your `llm_request_latency_seconds`, `rag_queries_total`, and other metrics so you donâ€™t have to build dashboards manually?